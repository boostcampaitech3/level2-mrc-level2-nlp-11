{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from konlpy.tag import Kkma, Mecab\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import time\n",
    "import json\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고\n",
    "https://amboulouma.com/elasticsearch-python\n",
    "\n",
    "https://gh402.tistory.com/51\n",
    "\n",
    "https://jvvp.tistory.com/1152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wikipedia json data\n",
    "with open('../../data/wikipedia_documents.json', 'r', encoding='utf-8') as f:\n",
    "    wiki = json.load(f)\n",
    "\n",
    "wiki_contents = list(dict.fromkeys([v['text'] for v in wiki.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data length: 60613\n",
      "W/o replicated data length: 56737\n"
     ]
    }
   ],
   "source": [
    "# Check wikipedia data\n",
    "print('Original data length:', len(wiki.keys()))\n",
    "print('W/o replicated data length:', len(wiki_contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch('http://localhost:30001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"iCfG3OW\",\n",
      "  \"cluster_name\" : \"elasticsearch\",\n",
      "  \"cluster_uuid\" : \"ne-ovptgRn-Gbd331afwZw\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"5.4.3\",\n",
      "    \"build_hash\" : \"eed30a8\",\n",
      "    \"build_date\" : \"2017-06-22T00:34:03.743Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"6.5.1\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test Elasticsearch Connection\n",
    "!curl -XGET localhost:30001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n",
      "1651647195 06:53:15  elasticsearch yellow          1         1     10  10    0    0       10             0                  -                 50.0%\n"
     ]
    }
   ],
   "source": [
    "# Check Cluster status\n",
    "!curl -XGET localhost:30001/_cat/health?v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health status index      uuid                   pri rep docs.count docs.deleted store.size pri.store.size\n",
      "yellow open   wiki_nouns skZkXThZTbaypb3JarTADg   5   1      56737            0     95.3mb         95.3mb\n",
      "yellow open   wiki       ufIaLl2KQGKrA5iL6hGx8Q   5   1      56737            0    189.3mb        189.3mb\n"
     ]
    }
   ],
   "source": [
    "# Get Index LIst\n",
    "!curl -XGET localhost:30001/_cat/indices?v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Index (이미 해당 index 가 존재하면 에러남)\n",
    "index = 'wiki_nouns'\n",
    "if not es.indices.exists(index):\n",
    "    es.indices.create(index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health status index      uuid                   pri rep docs.count docs.deleted store.size pri.store.size\n",
      "yellow open   wiki_nouns skZkXThZTbaypb3JarTADg   5   1          0            0       650b           650b\n",
      "yellow open   wiki       ufIaLl2KQGKrA5iL6hGx8Q   5   1      56737            0    189.3mb        189.3mb\n"
     ]
    }
   ],
   "source": [
    "# Get Index LIst  ->  생성한 index 가 보여야 함!\n",
    "!curl -XGET localhost:30001/_cat/indices?v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index 삭제\n",
    "# es.indices.delete(index='wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a data (Index should be existed!)\n",
    "# Only Do Once!\n",
    "\n",
    "kkma = Kkma()\n",
    "mecab = Mecab()\n",
    "\n",
    "# for idx, text in enumerate(wiki_contents):\n",
    "#     # body = {'text': ' '.join(kkma.nouns(text))}\n",
    "#     body = {'text': ' '.join(mecab.nouns(text))}\n",
    "#     # body = {'text': wiki_contents[idx]}\n",
    "#     es.index(index=index, doc_type=\"news\", id=idx+1, body=body)\n",
    "#     print(f'current: {idx}', end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'wiki_nouns',\n",
       " '_type': 'news',\n",
       " '_id': '56737',\n",
       " '_version': 1,\n",
       " 'found': True,\n",
       " '_source': {'text': '협약 부당 노동 행위 제도 규율 협약 조 반 노동조합 차별 행위 보호 규정 노동조합 가입 노동조합 탈퇴 것 조건 고용 황견계약 노동조합원 노동조합 활동 이유 이익 조치 것 보호 규정 조 노동자 단체 사용자 단체 사이 상호 간 간섭 보호 규정 사용 사용 단체 지배 하 둘 목적 노동자 단체 설립 지원 노동자 단체 재정 밖 방법 지원 것 간섭 행위 노동 조건 단체 협약 규율 사용 사용 단체 노동자 단체 사이 자발 교섭 기구 발전 이용 촉진 규정'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data\n",
    "es.get(index=index, doc_type='news', id=len(wiki_contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a data\n",
    "# es.delete(index=index, id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the data by Query (Main !!!!)\n",
    "# body = {\n",
    "#     'query': {\n",
    "#         'match': {\n",
    "#             'text': '유엔 국제 기구'\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# res = es.search(index=index, body=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['__index_level_0__', 'answers', 'context', 'document_id', 'id', 'question', 'title'],\n",
       "        num_rows: 3952\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['__index_level_0__', 'answers', 'context', 'document_id', 'id', 'question', 'title'],\n",
       "        num_rows: 240\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_dataset = load_from_disk('../../data/train_dataset')\n",
    "\n",
    "org_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f\"[{name}] done in {time.time() - t0:.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Validation Score: 72.5%\n",
      "[TOP K: 1] done in 1.195 s\n",
      "Total Validation Score: 78.75%\n",
      "[TOP K: 2] done in 1.365 s\n",
      "Total Validation Score: 80.83333333333333%\n",
      "[TOP K: 3] done in 1.472 s\n",
      "Total Validation Score: 82.91666666666667%\n",
      "[TOP K: 4] done in 1.610 s\n",
      "Total Validation Score: 83.75%\n",
      "[TOP K: 5] done in 1.711 s\n",
      "Total Validation Score: 85.83333333333333%\n",
      "[TOP K: 6] done in 1.759 s\n",
      "Total Validation Score: 87.91666666666667%\n",
      "[TOP K: 7] done in 1.883 s\n",
      "Total Validation Score: 89.16666666666667%\n",
      "[TOP K: 8] done in 1.947 s\n",
      "Total Validation Score: 90.41666666666667%\n",
      "[TOP K: 9] done in 2.038 s\n",
      "Total Validation Score: 90.41666666666667%\n",
      "[TOP K: 10] done in 2.170 s\n",
      "Total Validation Score: 90.83333333333333%\n",
      "[TOP K: 11] done in 2.182 s\n",
      "Total Validation Score: 90.83333333333333%\n",
      "[TOP K: 12] done in 2.221 s\n",
      "Total Validation Score: 90.83333333333333%\n",
      "[TOP K: 13] done in 2.258 s\n",
      "Total Validation Score: 90.83333333333333%\n",
      "[TOP K: 14] done in 2.307 s\n",
      "Total Validation Score: 91.25%\n",
      "[TOP K: 15] done in 2.373 s\n",
      "Total Validation Score: 92.08333333333333%\n",
      "[TOP K: 16] done in 2.418 s\n",
      "Total Validation Score: 92.08333333333333%\n",
      "[TOP K: 17] done in 2.462 s\n",
      "Total Validation Score: 92.5%\n",
      "[TOP K: 18] done in 2.525 s\n",
      "Total Validation Score: 92.5%\n",
      "[TOP K: 19] done in 2.603 s\n",
      "Total Validation Score: 92.5%\n",
      "[TOP K: 20] done in 2.631 s\n"
     ]
    }
   ],
   "source": [
    "# Inference for Train Validation\n",
    "\n",
    "for i in range(1, 21):\n",
    "    with timer(f'TOP K: {i}'):\n",
    "        TOPK = i\n",
    "        doc_scores = []\n",
    "        doc_indices = []\n",
    "        for j in range(len(org_dataset['validation'])):\n",
    "            # By match\n",
    "            body = {\n",
    "                'size': TOPK,\n",
    "                'query': {\n",
    "                    'match': {\n",
    "                        # 'text': ' '.join(kkma.nouns(org_dataset['validation']['question'][j]))\n",
    "                        'text': ' '.join(mecab.nouns(org_dataset['validation']['question'][j]))\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            \n",
    "            # By match_phrase  =>  Fail\n",
    "            # body = {\n",
    "            #     'size': TOPK,\n",
    "            #     'query': {\n",
    "            #         'match_phrase': {\n",
    "            #             # 'text': ' '.join(kkma.nouns(org_dataset['validation']['question'][j]))\n",
    "            #             'text': ' '.join(mecab.nouns(org_dataset['validation']['question'][j]))\n",
    "            #         }\n",
    "            #     }\n",
    "            # }\n",
    "\n",
    "\n",
    "            # By terms  =>  not better than match\n",
    "            # body = {\n",
    "            #     'size': TOPK,\n",
    "            #     'query': {\n",
    "            #         'bool': {\n",
    "            #             'should': [],\n",
    "            #             'minimum_should_match': 3\n",
    "            #         }\n",
    "            #     }\n",
    "            # }\n",
    "            # for noun in mecab.nouns(org_dataset['validation']['question'][j]):\n",
    "            #     body['query']['bool']['should'].append({'term': {'text': noun}})\n",
    "\n",
    "            res = es.search(index=index, body=body)\n",
    "\n",
    "            a_result_scores = []\n",
    "            a_result_indices = []\n",
    "\n",
    "            for item in res['hits']['hits']:\n",
    "                a_result_scores.append(item['_score'])\n",
    "                a_result_indices.append(int(item['_id'])-1)\n",
    "\n",
    "            doc_scores.append(a_result_scores)\n",
    "            doc_indices.append(a_result_indices)\n",
    "\n",
    "\n",
    "        # Context Accuracy\n",
    "\n",
    "        # correct = 0\n",
    "        # for idx, doc_indice in enumerate(doc_indices):\n",
    "        #     for jdx, indice in enumerate(doc_indice):\n",
    "        #         if org_dataset['validation']['context'][idx] == wiki_contents[indice]:\n",
    "        #             correct += 1\n",
    "\n",
    "        # Label Accuracy  =>  more reasonable ?\n",
    "        correct = 0\n",
    "        for idx, doc_indice in enumerate(doc_indices):\n",
    "            for jdx, indice in enumerate(doc_indice):\n",
    "                if org_dataset['validation']['answers'][idx]['text'][0] in wiki_contents[indice]:\n",
    "                    correct += 1\n",
    "                    break\n",
    "        \n",
    "        print(f\"Total Validation Score: {correct/len(org_dataset['validation'])*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
